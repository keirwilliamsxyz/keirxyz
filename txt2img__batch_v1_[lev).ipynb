{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keirwilliamsxyz/keirxyz/blob/main/txt2img__batch_v1_%5Blev).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL3qnIS2qC9K"
      },
      "source": [
        "# //**img2txt** Batch v0.1\n",
        "UI re-design of Multi-Perceptor VQGAN + CLIP by keir.xyz\n",
        "\n",
        "forked from:\n",
        "Multi-Perceptor VQGAN + CLIP\n",
        "[@remi_durant](https://twitter.com/remi_durant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKTnw-i7wH5E"
      },
      "source": [
        "#**Setup** VQGAN + Clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEAdjUn3RcNm"
      },
      "source": [
        "## **Install** VQGAN+Clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdC-w8aCQPAQ",
        "outputId": "2364cd26-7be5-4fc3-db81-b4983e931381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=1f13b5df8253fa211b4c2f370ef85cce8a107df435509088fe2ac4fc8e4007f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "#@title memory footprint support libraries/code\n",
        "\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZnX4Z6ScpKh",
        "outputId": "43d55b29-7dee-4c14-d856-90385933b29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan  4 21:08:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Gen RAM Free: 53.5 GB  |     Proc size: 120.1 MB\n",
            "GPU RAM Free: 16160MB | Used: 0MB | Util   0% | Total     16160MB\n"
          ]
        }
      ],
      "source": [
        "#@title Print GPU details\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b7nAR3tUgI6",
        "outputId": "9271d250-548f-415a-98dc-d441136610d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen RAM Free: 53.5 GB  |     Proc size: 120.1 MB\n",
            "GPU RAM Free: 16160MB | Used: 0MB | Util   0% | Total     16160MB\n"
          ]
        }
      ],
      "source": [
        "printm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zNIJyIYLQRsH",
        "outputId": "9e83a2c8-7378-4651-f115-7e02f1c8f927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.37.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 94.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 82.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.13.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.42.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.2) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=f1b9b56e3b231c43f11430397c476815573f8290c388da46d2d71e7f4b8ecc45\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 185 (delta 7), reused 13 (delta 4), pack-reused 168\u001b[K\n",
            "Receiving objects: 100% (185/185), 8.90 MiB | 15.67 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1297, done.\u001b[K\n",
            "remote: Counting objects: 100% (487/487), done.\u001b[K\n",
            "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
            "remote: Total 1297 (delta 32), reused 459 (delta 19), pack-reused 810\u001b[K\n",
            "Receiving objects: 100% (1297/1297), 409.22 MiB | 24.35 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.7-py3-none-any.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 98.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 88.5 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 87.3 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 96.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Collecting tensorboard>=2.2.0\n",
            "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 86.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 88.8 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 91.8 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 89.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 91.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.9)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 92.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 89.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Building wheels for collected packages: ftfy, antlr4-python3-runtime, future\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=b0ddf353cdac829680e51c78d982df5524cf2efb65739cc1edb7ff6fe8c13ba8\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=c54798696cca56c2096c316fb5d405ff53dcf82d9a02e759b9b7a9eee0b3d39f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=e2876aac2c282078a06d1f7de8cc29e6be9a115db6be8f7617c32cc7a49da2d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built ftfy antlr4-python3-runtime future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, tensorboard, PyYAML, pyDeprecate, future, antlr4-python3-runtime, pytorch-lightning, omegaconf, ftfy\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 1.15.2 requires tensorboard<1.16.0,>=1.15.0, but you have tensorboard 2.7.0 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 ftfy-6.0.3 future-0.18.2 multidict-5.2.0 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-lightning-1.5.7 tensorboard-2.7.0 torchmetrics-0.6.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.6.2-py2.py3-none-any.whl (401 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 41.2 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 143 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 153 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 174 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 184 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 204 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 215 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 235 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 245 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 256 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 266 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 286 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 296 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 307 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 317 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 327 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 337 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 348 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 358 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 368 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 378 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 399 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 401 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.6)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.2\n",
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 24.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 592 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install Dependencies\n",
        "\n",
        "# Fix for A100 issues\n",
        "!pip install tensorflow==1.15.2\n",
        "\n",
        "# Install normal dependencies\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "VtLgtENkRDk0"
      },
      "outputs": [],
      "source": [
        "#@title Load libraries and variables\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import os.path\n",
        "from os import path\n",
        "from urllib.request import Request, urlopen\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia\n",
        "import kornia.augmentation as K\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import random\n",
        "import gc\n",
        "\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "from base64 import b64encode\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.set_printoptions( sci_mode=False )\n",
        "\n",
        "def noise_gen(shape, octaves=5):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    max_octaves = min(octaves, math.log(h)/math.log(2), math.log(w)/math.log(2))\n",
        "    for i in reversed(range(max_octaves)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        \n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          \n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "  \n",
        "import io\n",
        "import base64\n",
        "def image_to_data_url(img, ext):  \n",
        "    img_byte_arr = io.BytesIO()\n",
        "    img.save(img_byte_arr, format=ext)\n",
        "    img_byte_arr = img_byte_arr.getvalue()\n",
        "    # ext = filename.split('.')[-1]\n",
        "    prefix = f'data:image/{ext};base64,'\n",
        "    return prefix + base64.b64encode(img_byte_arr).decode('utf-8')\n",
        " \n",
        "\n",
        "def update_random( seed, purpose ):\n",
        "  if seed == -1:\n",
        "    seed = random.seed()\n",
        "    seed = random.randrange(1,99999)\n",
        "    \n",
        "  print( f'Using seed {seed} for {purpose}')\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  return seed\n",
        "\n",
        "def clear_memory():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qg715v3EmEUZ"
      },
      "outputs": [],
      "source": [
        "#@title Setup for A100\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if gpu.name.startswith('A100'):\n",
        "  torch.backends.cudnn.enabled = False\n",
        "  print('Finished setup for A100')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "KDvGKLWVKqj1"
      },
      "outputs": [],
      "source": [
        "#@title Loss Module Definitions\n",
        "from typing import cast, Dict, Optional\n",
        "from kornia.augmentation.base import IntensityAugmentationBase2D\n",
        "\n",
        "class FixPadding(nn.Module):\n",
        "    \n",
        "    def __init__(self, module=None, threshold=1e-12, noise_frac=0.00 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.threshold = threshold\n",
        "        self.noise_frac = noise_frac\n",
        "\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self,input):\n",
        "\n",
        "        dims = input.shape\n",
        "\n",
        "        if self.module is not None:\n",
        "            input = self.module(input + self.threshold)\n",
        "\n",
        "        light = input.new_empty(dims[0],1,1,1).uniform_(0.,2.)\n",
        "\n",
        "        mixed = input.view(*dims[:2],-1).sum(dim=1,keepdim=True)\n",
        "\n",
        "        black = mixed < self.threshold\n",
        "        black = black.view(-1,1,*dims[2:4]).type(torch.float)\n",
        "        black = kornia.filters.box_blur( black, (5,5) ).clip(0,0.1)/0.1\n",
        "\n",
        "        mean = input.view(*dims[:2],-1).sum(dim=2) / mixed.count_nonzero(dim=2)\n",
        "        mean = ( mean[:,:,None,None] * light ).clip(0,1)\n",
        "\n",
        "        fill = mean.expand(*dims)\n",
        "        if 0 < self.noise_frac:\n",
        "            rng = torch.get_rng_state()\n",
        "            fill = fill + torch.randn_like(mean) * self.noise_frac\n",
        "            torch.set_rng_state(rng)\n",
        "        \n",
        "        if self.module is not None:\n",
        "            input = input - self.threshold\n",
        "\n",
        "        return torch.lerp(input,fill,black)\n",
        "\n",
        "\n",
        "class MyRandomNoise(IntensityAugmentationBase2D):\n",
        "    def __init__(\n",
        "        self,\n",
        "        frac: float = 0.1,\n",
        "        return_transform: bool = False,\n",
        "        same_on_batch: bool = False,\n",
        "        p: float = 0.5,\n",
        "    ) -> None:\n",
        "        super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0)\n",
        "        self.frac = frac\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__class__.__name__ + f\"({super().__repr__()})\"\n",
        "\n",
        "    def generate_parameters(self, shape: torch.Size) -> Dict[str, torch.Tensor]:\n",
        "        noise = torch.FloatTensor(1).uniform_(0,self.frac)\n",
        "        \n",
        "        # generate pixel data without throwing off determinism of augs\n",
        "        rng = torch.get_rng_state()\n",
        "        noise = noise * torch.randn(shape)\n",
        "        torch.set_rng_state(rng)\n",
        "\n",
        "        return dict(noise=noise)\n",
        "\n",
        "    def apply_transform(\n",
        "        self, input: torch.Tensor, params: Dict[str, torch.Tensor], transform: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        return input + params['noise'].to(input.device)\n",
        "\n",
        "class MakeCutouts2(nn.Module):\n",
        "    def __init__(self, cut_size, cutn):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          \n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(cutout)\n",
        "        \n",
        "        return cutouts\n",
        "\n",
        "\n",
        "class MultiClipLoss(nn.Module):\n",
        "    def __init__(self, clip_models, text_prompt, normalize_prompt_weights, cutn, cut_pow=1., clip_weight=1., use_old_augs=False, simulate_old_cuts=False ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_old_augs = use_old_augs\n",
        "        self.simulate_old_cuts = simulate_old_cuts \n",
        "\n",
        "        # Load Clip\n",
        "        self.perceptors = []\n",
        "        for cm in clip_models:\n",
        "          c = clip.load(cm[0], jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "          self.perceptors.append( { 'res': c.visual.input_resolution, 'perceptor': c, 'weight': cm[1], 'prompts':[] } )        \n",
        "        self.perceptors.sort(key=lambda e: e['res'], reverse=True)\n",
        "        \n",
        "        # Make Cutouts\n",
        "        self.cut_sizes = list(set([p['res'] for p in self.perceptors]))\n",
        "        self.cut_sizes.sort( reverse=True )\n",
        "        \n",
        "        self.make_cuts = MakeCutouts2(self.cut_sizes[-1], cutn)\n",
        "\n",
        "        # Get Prompt Embedings\n",
        "        texts = [phrase.strip() for phrase in text_prompt.split(\"|\")]\n",
        "        if text_prompt == ['']:\n",
        "          texts = []\n",
        "\n",
        "        self.pMs = []\n",
        "\n",
        "        prompts_weight_sum = 0\n",
        "        parsed_prompts = []\n",
        "        for prompt in texts:\n",
        "          txt, weight, stop = parse_prompt(prompt)\n",
        "          parsed_prompts.append( [txt,weight,stop] )\n",
        "          prompts_weight_sum += max( weight, 0 )\n",
        "\n",
        "        for prompt in parsed_prompts:\n",
        "          txt, weight, stop = prompt\n",
        "          clip_token = clip.tokenize(txt).to(device)\n",
        "\n",
        "          if normalize_prompt_weights and 0 < prompts_weight_sum:\n",
        "              weight /= prompts_weight_sum\n",
        "\n",
        "          for p in self.perceptors:\n",
        "            embed = p['perceptor'].encode_text(clip_token).float()\n",
        "            embed_normed = F.normalize(embed.unsqueeze(0), dim=2)\n",
        "            p['prompts'].append({'embed_normed':embed_normed,'weight':torch.as_tensor(weight, device=device),'stop':torch.as_tensor(stop, device=device)})\n",
        "    \n",
        "        # Prep Augments\n",
        "        self.noise_fac = 0.1\n",
        "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])        \n",
        "        \n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomSharpness(0.3,p=0.1),\n",
        "            FixPadding( nn.Sequential(\n",
        "                K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='zeros'), # padding_mode=2\n",
        "                K.RandomPerspective(0.2,p=0.4, ),\n",
        "            )),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "            K.RandomGrayscale(p=0.15), \n",
        "            MyRandomNoise(frac=self.noise_fac,p=1.),\n",
        "        )\n",
        "\n",
        "        self.clip_weight = clip_weight\n",
        "\n",
        "    def prepare_cuts(self,img):\n",
        "        cutouts = self.make_cuts(img)\n",
        "        cutouts_out = []\n",
        "            \n",
        "        rng = torch.get_rng_state()\n",
        "\n",
        "        for sz in self.cut_sizes:\n",
        "            cuts = [resample(c, (sz,sz)) for c in cutouts]\n",
        "            cuts = torch.cat(cuts, dim=0)\n",
        "            cuts = clamp_with_grad(cuts,0,1)\n",
        "\n",
        "            torch.set_rng_state(rng)\n",
        "            cuts = self.augs(cuts)\n",
        "            cuts = self.normalize(cuts)\n",
        "\n",
        "            cutouts_out.append(cuts)\n",
        "\n",
        "        return cutouts_out\n",
        "\n",
        "    def forward( self, i, img ):\n",
        "        cutouts = self.prepare_cuts( img )\n",
        "        loss = []\n",
        "        \n",
        "        current_cuts = None\n",
        "        currentres = 0\n",
        "        \n",
        "        for p in self.perceptors:\n",
        "            if currentres != p['res']:\n",
        "                currentres = p['res']\n",
        "                current_cuts = cutouts[self.cut_sizes.index( currentres )]\n",
        "\n",
        "            iii = p['perceptor'].encode_image(current_cuts).float()\n",
        "            input_normed = F.normalize(iii.unsqueeze(1), dim=2)\n",
        "            for prompt in p['prompts']:\n",
        "                dists = input_normed.sub(prompt['embed_normed']).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "                dists = dists * prompt['weight'].sign()\n",
        "                l = prompt['weight'].abs() * replace_grad(dists, torch.maximum(dists, prompt['stop'])).mean()\n",
        "                loss.append(l * p['weight'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "class MSEDecayLoss(nn.Module):\n",
        "    def __init__(self, init_weight, mse_decay_rate, mse_epoches, mse_quantize ):\n",
        "        super().__init__()\n",
        "      \n",
        "        self.init_weight = init_weight\n",
        "        self.has_init_image = False\n",
        "        self.mse_decay = init_weight / mse_epoches if init_weight else 0 \n",
        "        self.mse_decay_rate = mse_decay_rate\n",
        "        self.mse_weight = init_weight\n",
        "        self.mse_epoches = mse_epoches\n",
        "        self.mse_quantize = mse_quantize\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def set_target( self, z_tensor, model ):\n",
        "        z_tensor = z_tensor.detach().clone()\n",
        "        if self.mse_quantize:\n",
        "            z_tensor = vector_quantize(z_tensor.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n",
        "        self.z_orig = z_tensor\n",
        "          \n",
        "    def forward( self, i, z ):\n",
        "        if self.is_active(i):\n",
        "            return F.mse_loss(z, self.z_orig) * self.mse_weight / 2\n",
        "        return 0\n",
        "        \n",
        "    def is_active(self, i):\n",
        "        if not self.init_weight:\n",
        "          return False\n",
        "        if i <= self.mse_decay_rate and not self.has_init_image:\n",
        "          return False\n",
        "        return True\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step( self, i ):\n",
        "\n",
        "        if i % self.mse_decay_rate == 0 and i != 0 and i < self.mse_decay_rate * self.mse_epoches:\n",
        "            \n",
        "            if self.mse_weight - self.mse_decay > 0 and self.mse_weight - self.mse_decay >= self.mse_decay:\n",
        "              self.mse_weight -= self.mse_decay\n",
        "            else:\n",
        "              self.mse_weight = 0\n",
        "            print(f\"updated mse weight: {self.mse_weight}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "  \n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "agEFaX64bjdj"
      },
      "outputs": [],
      "source": [
        "#@title Random Inits\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def rand_perlin_2d(shape, res, fade = lambda t: 6*t**5 - 15*t**4 + 10*t**3):\n",
        "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
        "    d = (shape[0] // res[0], shape[1] // res[1])\n",
        "    \n",
        "    grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(0, res[1], delta[1])), dim = -1) % 1\n",
        "    angles = 2*math.pi*torch.rand(res[0]+1, res[1]+1)\n",
        "    gradients = torch.stack((torch.cos(angles), torch.sin(angles)), dim = -1)\n",
        "    \n",
        "    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat_interleave(d[0], 0).repeat_interleave(d[1], 1)\n",
        "    dot = lambda grad, shift: (torch.stack((grid[:shape[0],:shape[1],0] + shift[0], grid[:shape[0],:shape[1], 1] + shift[1]  ), dim = -1) * grad[:shape[0], :shape[1]]).sum(dim = -1)\n",
        "    \n",
        "    n00 = dot(tile_grads([0, -1], [0, -1]), [0,  0])\n",
        "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
        "    n01 = dot(tile_grads([0, -1],[1, None]), [0, -1])\n",
        "    n11 = dot(tile_grads([1, None], [1, None]), [-1,-1])\n",
        "    t = fade(grid[:shape[0], :shape[1]])\n",
        "    return math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1])\n",
        "\n",
        "def rand_perlin_2d_octaves( desired_shape, octaves=1, persistence=0.5):\n",
        "    shape = torch.tensor(desired_shape)\n",
        "    shape = 2 ** torch.ceil( torch.log2( shape ) )\n",
        "    shape = shape.type(torch.int)\n",
        "\n",
        "    max_octaves = int(min(octaves,math.log(shape[0])/math.log(2), math.log(shape[1])/math.log(2)))\n",
        "    res = torch.floor( shape / 2 ** max_octaves).type(torch.int)\n",
        "\n",
        "    noise = torch.zeros(list(shape))\n",
        "    frequency = 1\n",
        "    amplitude = 1\n",
        "    for _ in range(max_octaves):\n",
        "        noise += amplitude * rand_perlin_2d(shape, (frequency*res[0], frequency*res[1]))\n",
        "        frequency *= 2\n",
        "        amplitude *= persistence\n",
        "    \n",
        "    return noise[:desired_shape[0],:desired_shape[1]]\n",
        "\n",
        "def rand_perlin_rgb( desired_shape, amp=0.1, octaves=6 ):\n",
        "  r = rand_perlin_2d_octaves( desired_shape, octaves )\n",
        "  g = rand_perlin_2d_octaves( desired_shape, octaves )\n",
        "  b = rand_perlin_2d_octaves( desired_shape, octaves )\n",
        "  rgb = ( torch.stack((r,g,b)) * amp + 1 ) * 0.5\n",
        "  return rgb.unsqueeze(0).clip(0,1).to(device)\n",
        "\n",
        "\n",
        "def pyramid_noise_gen(shape, octaves=5, decay=1.):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    max_octaves = int(min(math.log(h)/math.log(2), math.log(w)/math.log(2)))\n",
        "    if octaves is not None and 0 < octaves:\n",
        "      max_octaves = min(octaves,max_octaves)\n",
        "    for i in reversed(range(max_octaves)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += ( torch.randn([n, c, h_cur, w_cur]) / max_octaves ) * decay**( max_octaves - (i+1) )\n",
        "    return noise\n",
        "\n",
        "def rand_z(model, toksX, toksY):\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "    return z\n",
        "\n",
        "\n",
        "def make_rand_init( mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f ):\n",
        "\n",
        "  if mode == 'VQGAN ZRand':\n",
        "    return rand_z(model, toksX, toksY)\n",
        "  elif mode == 'Perlin Noise':\n",
        "    rand_init = rand_perlin_rgb((toksY * f, toksX * f), perlin_weight, perlin_octaves )\n",
        "    z, *_ = model.encode(rand_init * 2 - 1)\n",
        "    return z\n",
        "  elif mode == 'Pyramid Noise':\n",
        "    rand_init = pyramid_noise_gen( (1,3,toksY * f, toksX * f), pyramid_octaves, pyramid_decay).to(device)\n",
        "    rand_init = ( rand_init * 0.5 + 0.5 ).clip(0,1)\n",
        "    z, *_ = model.encode(rand_init * 2 - 1)\n",
        "    return z\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93QxgHDB6jkr",
        "outputId": "3ba5c641-ee59-4a5b-b965-6a21e9961f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla V100-SXM2-16GB, 16160 MiB, 16158 MiB\n"
          ]
        }
      ],
      "source": [
        "#@title First check what GPU you got and make sure it's a good one. \n",
        "#@markdown - Tier List: (K80 < T4 < P100 < V100 < A100)\n",
        "from subprocess import getoutput\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BM04w60hxwq"
      },
      "source": [
        "## **Mount** Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxcx0j6zmePl",
        "outputId": "0e803a53-a5c3-4562-fff6-c16d3d159d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "save_vqgan_models_to_drive = True\n",
        "download_all = False \n",
        "vqgan_path_on_google_drive = \"/content/drive/MyDrive/art/data/models/vqgan\"\n",
        "vqgan_path_on_google_drive += \"/\" if not vqgan_path_on_google_drive.endswith('/') else \"\"\n",
        "save_output_to_drive = True\n",
        "output_path_on_google_drive = \"/content/drive/MyDrive/art/img/render/wev\" #@param {type: 'string'}\n",
        "output_path_on_google_drive += \"/\" if not output_path_on_google_drive.endswith('/') else \"\"\n",
        "include_full_prompt_in_filename = True\n",
        "shortname_limit =  15\n",
        "filename_limit = 250\n",
        "\n",
        "if save_vqgan_models_to_drive or save_output_to_drive:\n",
        "    from google.colab import drive    \n",
        "    drive._mount('/content/drive')\n",
        "\n",
        "vqgan_model_path = \"/content/\"\n",
        "if save_vqgan_models_to_drive:\n",
        "    vqgan_model_path = vqgan_path_on_google_drive\n",
        "    !mkdir -p \"$vqgan_path_on_google_drive\"\n",
        "\n",
        "save_output_path = \"/content/art/\"\n",
        "if save_output_to_drive:\n",
        "    save_output_path = output_path_on_google_drive\n",
        "!mkdir -p \"$save_output_path\"\n",
        "\n",
        "model_download={\n",
        "  \"vqgan_imagenet_f16_1024\":\n",
        "      [[\"vqgan_imagenet_f16_1024.yaml\", \"https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
        "      [\"vqgan_imagenet_f16_1024.ckpt\", \"https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1\"]],\n",
        "  \"vqgan_imagenet_f16_16384\": \n",
        "      [[\"vqgan_imagenet_f16_16384.yaml\", \"https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
        "      [\"vqgan_imagenet_f16_16384.ckpt\", \"https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1\"]],\n",
        "  \"vqgan_openimages_f8_8192\":\n",
        "      [[\"vqgan_openimages_f8_8192.yaml\", \"https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\"],\n",
        "      [\"vqgan_openimages_f8_8192.ckpt\", \"https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1\"]],\n",
        "  \"coco\":\n",
        "      [[\"coco_first_stage.yaml\", \"http://batbot.tv/ai/models/vqgan/coco_first_stage.yaml\"],\n",
        "      [\"coco_first_stage.ckpt\", \"http://batbot.tv/ai/models/vqgan/coco_first_stage.ckpt\"]],\n",
        "  \"faceshq\":\n",
        "      [[\"faceshq.yaml\", \"https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT\"],\n",
        "      [\"faceshq.ckpt\", \"https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt\"]],\n",
        "  \"wikiart_1024\":\n",
        "      [[\"wikiart_1024.yaml\", \"http://batbot.tv/ai/models/vqgan/WikiArt_augmented_Steps_7mil_finetuned_1mil.yaml\"],\n",
        "      [\"wikiart_1024.ckpt\", \"http://batbot.tv/ai/models/vqgan/WikiArt_augmented_Steps_7mil_finetuned_1mil.ckpt\"]],\n",
        "  \"wikiart_16384\":\n",
        "      [[\"wikiart_16384.yaml\", \"http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml\"],\n",
        "      [\"wikiart_16384.ckpt\", \"http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt\"]],\n",
        "  \"sflckr\":\n",
        "      [[\"sflckr.yaml\", \"https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1\"],\n",
        "      [\"sflckr.ckpt\", \"https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1\"]],\n",
        "  }\n",
        "\n",
        "loaded_model = None\n",
        "loaded_model_name = None\n",
        "def dl_vqgan_model(image_model):\n",
        "    for curl_opt in model_download[image_model]:\n",
        "        modelpath = f'{vqgan_model_path}{curl_opt[0]}'\n",
        "        if not path.exists(modelpath):\n",
        "            print(f'downloading {curl_opt[0]} to {modelpath}')\n",
        "            !curl -L -o {modelpath} '{curl_opt[1]}'\n",
        "        else:\n",
        "            print(f'found existing {curl_opt[0]}')\n",
        "\n",
        "def get_vqgan_model(image_model):\n",
        "    global loaded_model\n",
        "    global loaded_model_name\n",
        "    if loaded_model is None or loaded_model_name != image_model:\n",
        "        dl_vqgan_model(image_model)\n",
        "    \n",
        "        print(f'loading {image_model} vqgan checkpoint')\n",
        "\n",
        "        \n",
        "        vqgan_config= vqgan_model_path + model_download[image_model][0][0]\n",
        "        vqgan_checkpoint= vqgan_model_path + model_download[image_model][1][0]\n",
        "        print('vqgan_config',vqgan_config)\n",
        "        print('vqgan_checkpoint',vqgan_checkpoint)\n",
        "\n",
        "        model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
        "        if image_model == 'vqgan_openimages_f8_8192':\n",
        "            model.quantize.e_dim = 256\n",
        "            model.quantize.n_e = model.quantize.n_embed\n",
        "            model.quantize.embedding = model.quantize.embed\n",
        "\n",
        "        loaded_model = model\n",
        "        loaded_model_name = image_model\n",
        "\n",
        "    return loaded_model\n",
        "\n",
        "def slugify(value):\n",
        "    value = str(value)\n",
        "    value = re.sub(r':([-\\d.]+)', ' [\\\\1]', value)\n",
        "    value = re.sub(r'[|]','; ',value)\n",
        "    value = re.sub(r'[<>:\"/\\\\|?*]', ' ', value)\n",
        "    return value\n",
        "\n",
        "def get_filename(text, seed, i, ext):\n",
        "    if ( not include_full_prompt_in_filename ):\n",
        "        text = re.split(r'[|:;]',text, 1)[0][:shortname_limit]\n",
        "    text = slugify(text)\n",
        "\n",
        "    now = datetime.now()\n",
        "    t = now.strftime(\"%y%m%d%H%M\")\n",
        "    if i is not None:\n",
        "        data = f'; r{seed} i{i} {t}{ext}'\n",
        "    else:\n",
        "        data = f'; r{seed} {t}{ext}'\n",
        "\n",
        "    return text[:filename_limit-len(data)] + data\n",
        "\n",
        "def save_output(pil, text, seed, i):\n",
        "    fname = get_filename(text,seed,i,'.png')\n",
        "    pil.save(save_output_path + fname)\n",
        "\n",
        "if save_vqgan_models_to_drive and download_all:\n",
        "    for model in model_download.keys():\n",
        "        dl_vqgan_model(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH2Vh9G8fZyH"
      },
      "source": [
        "## Set Display Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm2zQEiQNo7Z"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "3Pvk8lfhfeCH"
      },
      "outputs": [],
      "source": [
        "use_automatic_display_schedule = True #@param {type:'boolean'}\n",
        "display_every =  10#@param {type:'number'}\n",
        "\n",
        "def should_checkin(i):\n",
        "  if i == max_iter: \n",
        "    return True \n",
        "\n",
        "  if not use_automatic_display_schedule:\n",
        "    return i % display_every == 0\n",
        "\n",
        "  schedule = [[100,25],[500,50],[1000,100],[2000,200]]\n",
        "  for s in schedule:\n",
        "    if i <= s[0]:\n",
        "      return i % s[1] == 0\n",
        "  return i % 500 == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO2PrvpgHtC7"
      },
      "source": [
        "\n",
        "# **Render** txt2img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FusxP0emsfhV"
      },
      "source": [
        "### Weaving img-05.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "de0c24fb86444f11a5140634347d82df",
            "f6840d130fb14b67ba8f76ac81fe1d7e",
            "820b41dc25724aff91daa71f410f5962",
            "58b986c0e1804e198e3b685df9eed6bd",
            "881230c841d545e9aa7cd43a01e57921",
            "d975ec62a3ef4c50894d62acad68de33",
            "3cb8000ebd0c40669ae6e78ed336f113",
            "8068b21197944b80bd91f85e83d58332",
            "93ac102c202b4aa1bf10be33a9007e12",
            "50a069bbce614c788dfac0dde2ff1f7a",
            "b124155006774c029249aa2a069091f6"
          ]
        },
        "id": "XAIwj1x3sfhV",
        "outputId": "fd14d222-0d72-435c-d814-a2b5c63847c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "a woven geometric patterns of repetition and invention:45 | Unreal Engine:10, 499 x 750, 1500i, 250 cut_n, 0.2 lr, 0.16 lre\n",
            "▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "Output to: /content/drive/MyDrive/art/img/render/wev/\n",
            "▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "Initial Image: /content/drive/MyDrive/art/img/int/wev/img-05.jpg\n",
            "▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "Gen RAM Free: 53.0 GB  |     Proc size: 1.3 GB\n",
            "GPU RAM Free: 16160MB | Used: 0MB | Util   0% | Total     16160MB\n",
            "▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
            "rm: cannot remove 'steps': No such file or directory\n",
            "found existing vqgan_imagenet_f16_16384.yaml\n",
            "found existing vqgan_imagenet_f16_16384.ckpt\n",
            "loading vqgan_imagenet_f16_16384 vqgan checkpoint\n",
            "vqgan_config /content/drive/MyDrive/art/data/models/vqgan/vqgan_imagenet_f16_16384.yaml\n",
            "vqgan_checkpoint /content/drive/MyDrive/art/data/models/vqgan/vqgan_imagenet_f16_16384.ckpt\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de0c24fb86444f11a5140634347d82df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vgg_lpips model from https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1 to taming/modules/autoencoder/lpips/vgg.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8.19kB [00:00, 1.18MB/s]                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-05.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqlzDgSsf2C"
      },
      "source": [
        "### Weaving img-06.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "gDeIMYB5sf2D"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-06.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS1THW0GsgJ6"
      },
      "source": [
        "### Weaving img-07.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "yGITiXelsgJ7"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-07.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6efXjXpbsgeD"
      },
      "source": [
        "### Weaving img-08.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "-9apSExisgeE"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-08.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHiQ5Scish_G"
      },
      "source": [
        "### Weaving img-09.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "DH9H6GwWsh_G"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-09.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSGdmv1_sik_"
      },
      "source": [
        "### Weaving img-10.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "1wIDyWbUsik_"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-10.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ_VQBvrsi8S"
      },
      "source": [
        "### Weaving img-11.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "Xsbvk4KJsi8T"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-11.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xBpjhtUsjSN"
      },
      "source": [
        "### Weaving img-12.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "zWBBtbJ-sjSN"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-12.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcEvytAjsjmu"
      },
      "source": [
        "### Weaving img-13.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "qONiN2eTsjmu"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-13.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MVnmflIsj-1"
      },
      "source": [
        "### Weaving img-14.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "1xFwT9ZTsj-1"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-14.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyzPUA7VskV8"
      },
      "source": [
        "### Weaving img-15.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "r-X4cAzuskV8"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-15.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "-ofNbHz3sk23"
      },
      "outputs": [],
      "source": [
        "    #@markdown  --------\n",
        "    #@markdown **Text Prompts**\n",
        "\n",
        "prompt1 = \"a woven geometric patterns of repetition and invention:45\" #@param {type:'string'}\n",
        "prompt2 = \"an entanglement of the creative personal and cosmological:45\" #@param {type:'string'}\n",
        "prompt3 = \"a woven Kinship and relational action:45\" #@param {type:'string'}\n",
        "prompt4 = \"a situated worlding both human and non-human:45\" #@param {type:'string'}\n",
        "prompt5 = \"a future that is tribal:45\" #@param {type:'string'}\n",
        "renderer = \"Unreal Engine\" #@param [\"illustrated by anthony browne\", \"houdini\", \"cinema4d\", \"cinema4d\" , \"zbrush\" , \"3Dmax\", \"octane\"] {allow-input: true}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Initial Image**\n",
        "\n",
        "#functions\n",
        "##UI functions\n",
        "\n",
        "def ui_line1():\n",
        "      ui_div_style = \"▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\" #param {type:'string'}\n",
        "      print(ui_div_style)\n",
        "\n",
        "\n",
        "text_list = [prompt1,prompt2,prompt3,prompt4,prompt5]\n",
        "    for text_cat in text_list:\n",
        "        text_prompt = text_prompt = text_cat + \" | \"+ renderer +\":10\"\n",
        "        gen_seed = -1#negative is random\n",
        "        init_image = '/content/drive/MyDrive/art/img/int/wev/img-01.jpg'#@param {type:'string'}\n",
        "        mse_weight =  45#@param {type: \"slider\", min: 0, max: 500}  \n",
        "    \n",
        "    #@markdown  --------\n",
        "    #@markdown **Image Size**\n",
        "    \n",
        "        width = 499#@param {type:'number'}\n",
        "        height = 750#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Iterations**\n",
        "\n",
        "        max_iter =  1500#@param {type:'number'}\n",
        "\n",
        "    #@markdown  --------\n",
        "    #@markdown **Learning**\n",
        "\n",
        "        #Perlin noise parameters\n",
        "        rand_init_mode = 'Pyramid Noise' # 'VQGAN ZRand' 'Perlin Noise', 'Pyramid Noise'\n",
        "        perlin_octaves = 7 #min:1, max:8, step:1\n",
        "        perlin_weight = 0.22#min:0, max:1, step:0.01\n",
        "        pyramid_octaves = 5#min:1, max:8, step:1\n",
        "        pyramid_decay = 1#min:0, max:1, step:0.01\n",
        "        ema_val = 0.99\n",
        "        normalize_prompt_weights = False\n",
        "\n",
        "        #Image Slices\n",
        "        cut_n =  250#@param {type:'number'}\n",
        "        clip_model = 'ViT-B/32' #\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        clip_model2 ='None' #None\",\"ViT-B/16\", \"ViT-B/32\", \"RN50x16\", \"RN50x4\"\n",
        "        if clip_model2 == \"None\":\n",
        "            clip_model2 = None \n",
        "        clip1_weight = 0.3 #min:0, max:1, step:0.01\n",
        "        vqgan_model = 'vqgan_imagenet_f16_16384'# [ \"vqgan_imagenet_f16_1024\", \"vqgan_imagenet_f16_16384\", \"vqgan_openimages_f8_8192\", \"coco\", \"faceshq\",\"wikiart_1024\", \"wikiart_16384\", \"sflckr\"]\n",
        "        learning_rate = 0.2#@param {type:'number'}\n",
        "        learning_rate_epoch = 0.16#@param {type:'number'}\n",
        "\n",
        "        #image weight\n",
        "        \n",
        "        tv_weight = 0.0\n",
        "        use_ema_tensor = False\n",
        "        save_art_output = True # {type:'boolean'}\n",
        "        save_frames_for_video = False # {type:'boolean'}\n",
        "        save_frequency_for_video =   2# {type:'number'}\n",
        "\n",
        "        #@markdown  --------\n",
        "\n",
        "        output_as_png = True\n",
        "\n",
        "        #Display render information\n",
        "        widthStr = str(width)\n",
        "        heightStr = str(height)\n",
        "        max_iterStr = str(max_iter)\n",
        "        cut_nStr = str(cut_n)\n",
        "        learning_rateStr = str(learning_rate)\n",
        "        learning_rate_epochStr = str(learning_rate_epoch)\n",
        "        ui_line1()\n",
        "        print(text_prompt + \", \" + widthStr + \" x \" + heightStr + \", \" + max_iterStr + \"i, \" + cut_nStr + \" cut_n, \" + learning_rateStr + \" lr, \" + learning_rate_epochStr + \" lre\")\n",
        "        ui_line1()\n",
        "        print(\"Output to: \" + output_path_on_google_drive)\n",
        "        ui_line1()\n",
        "        print(\"Initial Image: \" + init_image)\n",
        "        ui_line1() \n",
        "        clear_memory()\n",
        "        #print Memory/ GPU info\n",
        "        printm()\n",
        "        ui_line1() \n",
        "\n",
        "        !rm -r steps\n",
        "        !mkdir -p steps\n",
        "\n",
        "        model = get_vqgan_model( vqgan_model )\n",
        "\n",
        "        if clip_model2:\n",
        "          clip_models = [[clip_model, clip1_weight], [clip_model2, 1. - clip1_weight]]\n",
        "        else:\n",
        "          clip_models = [[clip_model, 1.0]]\n",
        "\n",
        "\n",
        "        clip_loss = MultiClipLoss( clip_models, text_prompt, normalize_prompt_weights=normalize_prompt_weights, cutn=cut_n)\n",
        "\n",
        "        seed = update_random( gen_seed, 'image generation')\n",
        "\n",
        "        # Make Z Init\n",
        "        z = 0\n",
        "\n",
        "        f = 2**(model.decoder.num_resolutions - 1)\n",
        "        toksX, toksY = math.ceil( width / f), math.ceil( height / f)\n",
        "\n",
        "        #print(f'Outputing size: [{toksX*f}x{toksY*f}]')\n",
        "\n",
        "        has_init_image = (init_image != \"\")\n",
        "        if has_init_image:\n",
        "            if 'http' in init_image:\n",
        "              req = Request(init_image, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "              img = Image.open(urlopen(req))\n",
        "            else:\n",
        "              img = Image.open(init_image)\n",
        "\n",
        "            pil_image = img.convert('RGB')\n",
        "            pil_image = pil_image.resize((toksX * f, toksY * f), Image.LANCZOS)\n",
        "            pil_image = TF.to_tensor(pil_image)\n",
        "            #if args.use_noise:\n",
        "            #  pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "            z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "            del pil_image\n",
        "            del img\n",
        "\n",
        "        else:\n",
        "            z = make_rand_init( rand_init_mode, model, perlin_octaves, perlin_weight, pyramid_octaves, pyramid_decay, toksX, toksY, f )\n",
        "\n",
        "        z = EMATensor(z, ema_val)\n",
        "\n",
        "        opt = optim.Adam( z.parameters(), lr=learning_rate, weight_decay=0.00000000)\n",
        "\n",
        "        mse_loss = MSEDecayLoss( mse_weight, mse_decay_rate=50, mse_epoches=5, mse_quantize=True )\n",
        "        mse_loss.set_target( z.tensor, model )\n",
        "        mse_loss.has_init_image = has_init_image\n",
        "\n",
        "        tv_loss = TVLoss() \n",
        "\n",
        "\n",
        "        losses = []\n",
        "        mb = master_bar(range(1))\n",
        "        gnames = ['losses']\n",
        "\n",
        "        mb.names=gnames\n",
        "        mb.graph_fig, axs = plt.subplots(1, 1) # For custom display\n",
        "        mb.graph_ax = axs\n",
        "        mb.graph_out = display.display(mb.graph_fig, display_id=True)\n",
        "\n",
        "        ## optimizer loop\n",
        "\n",
        "        def synth(z, quantize=True, scramble=True):\n",
        "            z_q = 0\n",
        "            if quantize:\n",
        "              z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "            else:\n",
        "              z_q = z.model\n",
        "\n",
        "            out = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def checkin(i, z, out_pil, losses):\n",
        "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "            tqdm.write(f'Iteration #: {i}') #loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "            display_format='png' if output_as_png else 'jpg'\n",
        "            pil_data = image_to_data_url(out_pil, display_format)\n",
        "\n",
        "            display.display(display.HTML(f'<img src=\"{pil_data}\" />'))\n",
        "\n",
        "        def should_save_for_video(i):\n",
        "            return save_frames_for_video and i % save_frequency_for_video\n",
        "\n",
        "        def train(i):\n",
        "            global opt\n",
        "            global z \n",
        "            opt.zero_grad( set_to_none = True )\n",
        "\n",
        "            out = checkpoint( synth, z.tensor )\n",
        "\n",
        "            lossAll = []\n",
        "            lossAll += clip_loss( i,out )\n",
        "\n",
        "            if 0 < mse_weight:\n",
        "              msel = mse_loss(i,z.tensor)\n",
        "              if 0 < msel:\n",
        "                lossAll.append(msel)\n",
        "\n",
        "            if 0 < tv_weight:\n",
        "              lossAll.append(tv_loss(out)*tv_weight)\n",
        "\n",
        "            loss = sum(lossAll)\n",
        "            loss.backward()\n",
        "\n",
        "            if should_checkin(i) or should_save_for_video(i):\n",
        "                with torch.no_grad():\n",
        "                    if use_ema_tensor:\n",
        "                        out = synth( z.average )\n",
        "\n",
        "                    pil = TF.to_pil_image(out[0].cpu())\n",
        "\n",
        "                    if should_checkin(i):\n",
        "                        checkin(i, z, pil, lossAll)\n",
        "                        if save_art_output:\n",
        "                            save_output(pil, text_prompt, seed, i)\n",
        "\n",
        "                    if should_save_for_video(i):\n",
        "                        pil.save(f'steps/step{i//save_frequency_for_video:04}.png')\n",
        "\n",
        "            # update graph\n",
        "            losses.append(loss)\n",
        "            x = range(len(losses))\n",
        "            mb.update_graph( [[x,losses]] )\n",
        "\n",
        "            opt.step()\n",
        "            if use_ema_tensor:\n",
        "              z.update()\n",
        "\n",
        "        i = 0\n",
        "        try:\n",
        "          with tqdm() as pbar:\n",
        "            while True and i <= max_iter:\n",
        "\n",
        "              if i % 200 == 0:\n",
        "                clear_memory()\n",
        "\n",
        "              train(i)\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  if mse_loss.step(i):\n",
        "                      print('Reseting optimizer at mse epoch')\n",
        "\n",
        "                      if mse_loss.has_init_image and use_ema_tensor:\n",
        "                        mse_loss.set_target(z.average,model)\n",
        "                      else:\n",
        "                        mse_loss.set_target(z.tensor,model)\n",
        "\n",
        "                      # Make sure not to spike loss when mse_loss turns on\n",
        "                      if not mse_loss.is_active(i):\n",
        "                        z.tensor = nn.Parameter(mse_loss.z_orig.clone())\n",
        "                        z.tensor.requires_grad = True\n",
        "\n",
        "                      if use_ema_tensor:\n",
        "                        z = EMATensor(z.average, ema_val)\n",
        "                      else:\n",
        "                        z = EMATensor(z.tensor, ema_val)\n",
        "\n",
        "                      opt = optim.Adam(z.parameters(), lr=learning_rate_epoch, weight_decay=0.00000000)\n",
        "\n",
        "              i += 1\n",
        "              pbar.update()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "uKTnw-i7wH5E",
        "NEAdjUn3RcNm",
        "4BM04w60hxwq",
        "vH2Vh9G8fZyH",
        "FusxP0emsfhV",
        "xVqlzDgSsf2C",
        "iS1THW0GsgJ6",
        "6efXjXpbsgeD",
        "LHiQ5Scish_G"
      ],
      "machine_shape": "hm",
      "name": "txt2img__batch_v1_[lev).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de0c24fb86444f11a5140634347d82df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f6840d130fb14b67ba8f76ac81fe1d7e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_820b41dc25724aff91daa71f410f5962",
              "IPY_MODEL_58b986c0e1804e198e3b685df9eed6bd",
              "IPY_MODEL_881230c841d545e9aa7cd43a01e57921"
            ]
          }
        },
        "f6840d130fb14b67ba8f76ac81fe1d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "820b41dc25724aff91daa71f410f5962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d975ec62a3ef4c50894d62acad68de33",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3cb8000ebd0c40669ae6e78ed336f113"
          }
        },
        "58b986c0e1804e198e3b685df9eed6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8068b21197944b80bd91f85e83d58332",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93ac102c202b4aa1bf10be33a9007e12"
          }
        },
        "881230c841d545e9aa7cd43a01e57921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_50a069bbce614c788dfac0dde2ff1f7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:03&lt;00:00, 190MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b124155006774c029249aa2a069091f6"
          }
        },
        "d975ec62a3ef4c50894d62acad68de33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3cb8000ebd0c40669ae6e78ed336f113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8068b21197944b80bd91f85e83d58332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93ac102c202b4aa1bf10be33a9007e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50a069bbce614c788dfac0dde2ff1f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b124155006774c029249aa2a069091f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}