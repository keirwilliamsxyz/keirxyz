{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VQGAN+CLIP Batch generate + canvas",
      "provenance": [],
      "collapsed_sections": [
        "10c0088e",
        "aef5ef96",
        "638cbf00",
        "8063afa3",
        "546e4e6e",
        "d883010e",
        "44bb00b5",
        "579050bd",
        "531c03f3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keirwilliamsxyz/keirxyz/blob/main/VQGAN%2BCLIP_Batch_generate_%2B_canvas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "db1a79e7"
      },
      "source": [
        "# Generating images using VQGAN+CLIP (+ mass generating)\n",
        "\n",
        "Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings).\n",
        "The original BigGAN+CLIP method was by https://twitter.com/advadnoun.\n",
        "Added some explanations and modifications by Eleiber#8347, pooling trick by Crimeacs#8222 (https://twitter.com/EarthML1) and the GUI was made with the help of Abulafia#3734."
      ],
      "id": "db1a79e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "5cc22914"
      },
      "source": [
        "This notebook adds to the original in the fact, that it facilitates multiple runs of generating images based on a list of prompts. We're using it in some of our experiments with the prompts and inputs - and perhaps you may find use of it too! If you see any ways to improve it or see mistakes, please do let me know: https://twitter.com/neurowelt :)"
      ],
      "id": "5cc22914"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "10c0088e",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Licensed uner the MIT License"
      ],
      "id": "10c0088e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false,
          "source_hidden": true
        },
        "id": "a12d1a70",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "source": [
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE.\n"
      ],
      "id": "a12d1a70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "aef5ef96",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Installations\n",
        "\n",
        "If you are using permanent storage of any sort then you do not have to repeat this step.\n",
        "\n",
        "(If on Colab you're asked to restart runtime to update `pydevd` package - you don't have to do that, I found it actually breaking the generation process)"
      ],
      "id": "aef5ef96"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false,
          "source_hidden": true
        },
        "id": "0b199623",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "source": [
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers.git\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install imageio-ffmpeg   \n",
        "!pip install einops\n",
        "!pip install psutil"
      ],
      "id": "0b199623",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "3f81d273",
        "tags": []
      },
      "source": [
        "## Set up necessary modules"
      ],
      "id": "3f81d273"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "638cbf00",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Libraries"
      ],
      "id": "638cbf00"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false,
          "source_hidden": true
        },
        "id": "fbe5b2ba",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import psutil\n",
        "import random\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sys.path.insert(1, 'taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import taming.modules \n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.augs = nn.Sequential(\n",
        "            # K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomVerticalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            # K.RandomSharpness(0.3,p=0.4),\n",
        "            # K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5),\n",
        "            # K.RandomCrop(size=(self.cut_size,self.cut_size), p=0.5),\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
        "            K.RandomPerspective(0.7,p=0.7),\n",
        "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
        "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n",
        "            \n",
        ")\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        \n",
        "        for _ in range(self.cutn):\n",
        "\n",
        "            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            # offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            # offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            # cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "            # cutout = transforms.Resize(size=(self.cut_size, self.cut_size))(input)\n",
        "            \n",
        "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
        "            cutouts.append(cutout)\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "id": "fbe5b2ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "8063afa3",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Google Drive\n",
        "\n",
        "If you want to connect your drive for accessing your models or using the option to save final iteration of each image to Google Drive, connect you drive here."
      ],
      "id": "8063afa3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false,
          "source_hidden": true
        },
        "id": "787db793",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "source": [
        "### COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "787db793",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "6e4e410a"
      },
      "source": [
        "## Prepare data and parameters"
      ],
      "id": "6e4e410a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "61ce5a99",
        "tags": []
      },
      "source": [
        "### Input and output data"
      ],
      "id": "61ce5a99"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "4f8d8baf",
        "tags": []
      },
      "source": [
        "#### Input data by hand\n",
        "\n",
        "You can easily just write down all the data you want to use for generation below.\n",
        "\n",
        "* `texts` - write prompts for generation here, separating them with a `|`\n",
        "* `input_images` - write the names in square brackets of initial image files on which you would like to generate images, separate them with `|`\n",
        "* `iterations` - write down iterations in square brakcets for each image, separating them with `|`\n",
        "\n",
        "Each prompt will be paired with input image name and iteration on the corresponding position. If you would like to generate a given prompt on more than one image and on more iterations, write as follows (an example):\n",
        "* `texts`: \"black sheep by Picasso | red apple on a tree\"\n",
        "* `input_images`: \"[b.png] | [c.png, b.png]\"\n",
        "* `iterations`: [20, 30, 40] | [20]\n",
        "\n",
        "So you get:\n",
        "* black sheep by Picasso – [20, 30, 40] – [b.png]\n",
        "* red apple on a tree – [20] – [c.png, b.png]"
      ],
      "id": "4f8d8baf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "08198791",
        "cellView": "form"
      },
      "source": [
        "texts = \"\" #@param {type:\"string\"}\n",
        "input_images = \"\" #@param {type:\"string\"}\n",
        "iterations = \"\" #@param {type:\"string\"}\n",
        "\n",
        "texts = [phrase.strip() for phrase in texts.split(\"|\")]\n",
        "iterations = [phrase.strip()[1:-1] for phrase in iterations.split(\"|\")]\n",
        "input_images = [phrase.strip()[1:-1] for phrase in input_images.split(\"|\")]\n",
        "\n",
        "way_chosen = 1\n",
        "prompts = pd.DataFrame({\"prompt\":texts,\"filename\":input_images,\"iterations\":iterations})"
      ],
      "id": "08198791",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "ebf169ed",
        "tags": []
      },
      "source": [
        "#### Import data from .xlsx\n",
        "\n",
        "You can import data from .xlsx if you want to generate in a more factory fashion. You can choose one of two ways: the **artist+prompt** way or the **all-in-one** way."
      ],
      "id": "ebf169ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "258b8690",
        "tags": []
      },
      "source": [
        "##### Artists and prompts way\n",
        "\n",
        "You will need two .xlsx files in order to proceed with this method. The first one with just the artists, the seond one with all the rest of the prompts.\n",
        "\n",
        "Artists excel should have only one column with artists (there may be many artists in one cell).\n",
        "\n",
        "Prompts should be constructed in this way:\n",
        "* `prompt` - text used for generating\n",
        "* `filename` - the names of the input files separated with `, `\n",
        "* `iterations` - iterations for each prompted image, there may be just one, but you can write them down in the same manner as filenames, with `,`\n",
        "\n",
        "Then, the final prompts will be generated in a form:\n",
        "\"{`prompt`} by {`artist`}\" and will be created for all possible combinations between artists and prompts.\n",
        "\n",
        "This will create prompts for every artist and prompt, giving you  `len(artists) * len(prompts)`  of generations."
      ],
      "id": "258b8690"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "4780f3b5",
        "tags": [],
        "cellView": "form"
      },
      "source": [
        "artists_excel = \"\" #@param {type:\"string\"}\n",
        "prompts_excel = \"\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "    arts = pd.read_excel('your_excel.xlsx')#,header=[1],index_col=0) # sometimes pandas adds Unnamed column, that helps fixing it\n",
        "    other = pd.read_excel('your_excel.xlsx')\n",
        "    data_a = pd.DataFrame(np.where(arts.isna(),'',arts),columns=['artist'])\n",
        "\n",
        "    data_o = pd.DataFrame(np.where(other.isna(),'',other),columns=['prompt','filename','iterations'])\n",
        "\n",
        "    prompts = data_o.fillna('')\n",
        "    artists = data_a.fillna('').sort_index()\n",
        "    way_chosen = 0\n",
        "    print(artists.head())\n",
        "    print(prompts.head())\n",
        "except Exception as e:\n",
        "    print(f'Failed to load dataset because of a following exception: {e}')"
      ],
      "id": "4780f3b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "9dad1dec"
      },
      "source": [
        "##### All-in-one way\n",
        "\n",
        "You will need just one excel spreadsheet filled with prompts. In order for this spreadsheet to work it requires to have the following columns:\n",
        "* `prompt` - text used for generating\n",
        "* `filename` - name of the file to base the generated image on (if no input then leave the cell empty), there can be multiple files, if so please separate with `,`\n",
        "* `iterations` - iterations for each prompted image, you can put many iterations for given image, just separate them with `,`"
      ],
      "id": "9dad1dec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "6e028eab",
        "cellView": "form"
      },
      "source": [
        "your_excel = \"\" #@param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "    d = pd.read_excel(your_excel)\n",
        "    prompts = pd.DataFrame(np.where(d.isna(),'',d),columns=['prompt','filename','iterations'])\n",
        "    \n",
        "    way_chosen = 1\n",
        "except Exception as e:\n",
        "    print(f'Failed to load dataset because of a following exception: {e}')"
      ],
      "id": "6e028eab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "546e4e6e",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "#### Output folders\n",
        "\n",
        "Create folders for storing generated images, by default it will create a steps/full folder. **If steps folder exists, it will be deleted!**"
      ],
      "id": "546e4e6e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "6dd2734f",
        "tags": []
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "path0 = './steps'\n",
        "path1 = './steps/full'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(path0)\n",
        "    os.mkdir(path0)\n",
        "    os.mkdir(path1)\n",
        "except OSError:\n",
        "    try:\n",
        "        os.mkdir(path0)\n",
        "        os.mkdir(path1)\n",
        "    except OSError as e:\n",
        "        print(\"Failed: {}\".format(e))"
      ],
      "id": "6dd2734f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "9a5fa00e"
      },
      "source": [
        "### Parameters"
      ],
      "id": "9a5fa00e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "aa4a6589"
      },
      "source": [
        "First generate a random seed. If set to $-1$ a random seed will be generated."
      ],
      "id": "aa4a6589"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3621aa4",
        "cellView": "form"
      },
      "source": [
        "seed =  -1#@param {type:\"number\", min:0}\n",
        "\n",
        "if seed == -1:\n",
        "    random.seed(int(str((psutil.virtual_memory()[0] / psutil.virtual_memory()[1]))[-3:]))\n",
        "    seed = random.randint(0,1000)"
      ],
      "id": "c3621aa4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "d883010e",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "#### Model choice\n",
        "\n",
        "Choose the model to use. Choose accordingly to the platform you're on. As in the original notebook, possibility of donwloading the model below."
      ],
      "id": "d883010e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false,
          "source_hidden": true
        },
        "id": "a3152815",
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        "#@markdown By default, the notebook downloads the 1024 and 16384 models from ImageNet. There are others like COCO-Stuff, WikiArt 1024, WikiArt 16384, FacesHQ or S-FLCKR, which are heavy, and if you are not going to use them it would be pointless to download them, so if you want to use them, simply select the models to download.\n",
        "\n",
        "imagenet_1024 = True #@param {type:\"boolean\"}\n",
        "imagenet_16384 = False #@param {type:\"boolean\"}\n",
        "coco = False #@param {type:\"boolean\"}\n",
        "faceshq = False #@param {type:\"boolean\"}\n",
        "wikiart_1024 = False #@param {type:\"boolean\"}\n",
        "wikiart_16384 = False #@param {type:\"boolean\"}\n",
        "sflckr = False #@param {type:\"boolean\"}\n",
        "openimages_8192 = False #@param {type:\"boolean\"}\n",
        "\n",
        "if imagenet_1024:\n",
        "  #!curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_1024.yaml' #ImageNet 1024\n",
        "  #!curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_1024.ckpt'  #ImageNet 1024\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1'\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'\n",
        "if imagenet_16384:\n",
        "  #!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.yaml' #ImageNet 16384\n",
        "  #!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.ckpt' #ImageNet 16384\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt' #ImageNet 16384\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml' #ImageNet 16384\n",
        "if openimages_8192:\n",
        "  !curl -L -o vqgan_openimages_f16_8192.yaml -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
        "  !curl -L -o vqgan_openimages_f16_8192.ckpt -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n",
        "\n",
        "if coco:\n",
        "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
        "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
        "if faceshq:\n",
        "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
        "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
        "if wikiart_1024: \n",
        "  !curl -L -o wikiart_1024.yaml -C - 'http://mirror.io.community/blob/vqgan/wikiart.yaml' #WikiArt 1024\n",
        "  !curl -L -o wikiart_1024.ckpt -C - 'http://mirror.io.community/blob/vqgan/wikiart.ckpt' #WikiArt 1024\n",
        "if wikiart_16384: \n",
        "  !curl -L -o wikiart_16384.yaml -C - 'http://mirror.io.community/blob/vqgan/wikiart_16384.yaml' #WikiArt 16384\n",
        "  !curl -L -o wikiart_16384.ckpt -C - 'http://mirror.io.community/blob/vqgan/wikiart_16384.ckpt' #WikiArt 16384\n",
        "if sflckr:\n",
        "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
        "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR"
      ],
      "id": "a3152815",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "e81bf6e1"
      },
      "source": [
        "And option for locally stored model."
      ],
      "id": "e81bf6e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "25efb8ca"
      },
      "source": [
        "### COLAB\n",
        "LOCAL_MODEL_YAML = 'your.yaml'\n",
        "LOCAL_MODEL_CKPT = 'your.ckpt'"
      ],
      "id": "25efb8ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "910e99dd"
      },
      "source": [
        "#### Model parameters\n",
        "\n",
        "Set up the parameters and prepare the final settings list for the model.\n",
        "\n",
        "Following parameters can be changed below:\n",
        "* `width` and `height` - regulate the size of the image\n",
        "* `model` - choose model based on the one you picked before\n",
        "* `target_images` - choose the target image for the generator (leave empty if not used)\n",
        "* `seed` - the generator's seed, if the same allows for comparison between outcomes of generations\n",
        "* `step_size` - effectively the size of generation steps, the larger it is the further the generation process goes witihin the same time span\n",
        "* `local_model` - if `True` than will use a model available in the storage the notebook is on (depends on what you picked before)"
      ],
      "id": "910e99dd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "d1a5cacf",
        "tags": [],
        "cellView": "form"
      },
      "source": [
        "#@title Parameters\n",
        "width =  512 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "model = \"vqgan_imagenet_f16_16384\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"vqgan_openimages_f16_8192\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n",
        "target_images = \"\" #@param {type:\"string\"}\n",
        "step_size = 0.1 #@param {type:\"slider\", min:0.001, max:0.9, step:0.001}\n",
        "local_model = True #@param {type: \"boolean\"}\n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "\n",
        "if target_images == \"None\" or not target_images:\n",
        "    target_images = []\n",
        "else:\n",
        "    target_images = target_images.split(\"|\")\n",
        "    target_images = [image.strip() for image in target_images]\n",
        "\n",
        "if local_model:\n",
        "    model_yaml = LOCAL_MODEL_YAML\n",
        "    model_ckpt = LOCAL_MODEL_CKPT\n",
        "else:\n",
        "    model_yaml = f'{model}.yaml'\n",
        "    model_ckpt = f'{model}.ckpt'\n",
        "    \n",
        "\n",
        "if way_chosen == 1:\n",
        "    try:\n",
        "        row_tracker = 0\n",
        "        multiple_runs = []\n",
        "        \n",
        "        for prompt in prompts.prompt:\n",
        "            texts = []\n",
        "            this_row = prompts.loc[prompts['prompt']==prompt]\n",
        "            \n",
        "            try:\n",
        "                iters = [int(x.strip()) for x in this_row.iloc[0,-1].split(\",\")]\n",
        "            except ValueError:\n",
        "                print('No numerical values in the \"iterations\" column. Please repair.')\n",
        "            except AttributeError:\n",
        "                iters = [this_row.iloc[0,-1]]\n",
        "                \n",
        "            init_image = [x.strip() for x in this_row.iloc[0,-2].split(\",\")]\n",
        "            if init_image == '':\n",
        "                init_image = ['None']\n",
        "                \n",
        "            texts.append(\"{prompt}\".format(prompt=prompt))\n",
        "            \n",
        "            settings = AttrDict({'texts':texts,\n",
        "                                 'width':width,\n",
        "                                 'height':height,\n",
        "                                 'model':[model_yaml, model_ckpt, model],\n",
        "                                 'images_interval':1,\n",
        "                                 'init_images':init_image,\n",
        "                                 'target_images':target_images,\n",
        "                                 'seed':seed,\n",
        "                                 'max_iterations':iters[-1],\n",
        "                                 'iter_stops':iters,\n",
        "                                 'step_size':step_size,\n",
        "                                 'id':[row_tracker],\n",
        "                                 })\n",
        "            \n",
        "            row_tracker += 1\n",
        "            multiple_runs.append(settings)\n",
        "    except NameError:\n",
        "        print(\"Artists dataframe doesn't exist\")\n",
        "    except IndexError:\n",
        "        print(\"Artists dataframe is empty\")\n",
        "    \n",
        "\n",
        "if way_chosen == 0:\n",
        "    try:\n",
        "        multiple_runs = []\n",
        "\n",
        "        for artist in artists.artist:\n",
        "            texts = []\n",
        "            row_ids = []\n",
        "            inits = []\n",
        "            row_tracker = 0\n",
        "\n",
        "            for prompt in prompts.prompt:\n",
        "                this_row = prompts.iloc[row_tracker]\n",
        "\n",
        "                try:\n",
        "                    iters = [int(x.strip()) for x in this_row[-1].split(\",\")]\n",
        "                except ValueError:\n",
        "                    print('No numerical values in the \"iterations\" column. Please repair.')\n",
        "                except AttributeError:\n",
        "                    iters = [this_row[-1]]\n",
        "\n",
        "                init_image = [x.strip() for x in this_row[-2].split(\",\")]\n",
        "                if init_image == '':\n",
        "                    init_image = ['None']\n",
        "                inits.append(init_image)\n",
        "\n",
        "                text = this_row[0]\n",
        "\n",
        "                if ((text != '')):\n",
        "                    for i in range(len(init_image)):\n",
        "                        texts.append(\"{text} by {art}\".format(text=text,\n",
        "                                                              art=artist))\n",
        "                        row_ids.append(row_tracker)\n",
        "\n",
        "                row_tracker += 1\n",
        "\n",
        "            inits = list(itertools.chain(*inits))\n",
        "            settings = AttrDict({'texts':texts,\n",
        "                                 'width':width,\n",
        "                                 'height':height,\n",
        "                                 'model':[model_yaml, model_ckpt, model],\n",
        "                                 'images_interval':1,\n",
        "                                 'init_images':inits,\n",
        "                                 'target_images':target_images,\n",
        "                                 'seed':seed,\n",
        "                                 'max_iterations':iters[-1],\n",
        "                                 'iter_stops':iters,\n",
        "                                 'step_size':step_size,\n",
        "                                 'id':row_ids,\n",
        "                                 })\n",
        "\n",
        "            multiple_runs.append(settings)\n",
        "    except NameError:\n",
        "        print(\"Artists dataframe doesn't exist\")\n",
        "    except IndexError:\n",
        "        print(\"Artists dataframe is empty\")"
      ],
      "id": "d1a5cacf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "44bb00b5"
      },
      "source": [
        "### Prepare model and generation procedure"
      ],
      "id": "44bb00b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "579050bd"
      },
      "source": [
        "#### Function preparing the model"
      ],
      "id": "579050bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "89ea225d",
        "tags": []
      },
      "source": [
        "def prepare_model(settings):\n",
        "    model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", 'vqgan_openimages_f16_8192':'OpenImages 8912',\n",
        "                    \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\n",
        "    name_model = model_names[settings.model[2]]     \n",
        "\n",
        "    args = argparse.Namespace(\n",
        "        prompts=[settings.texts],\n",
        "        image_prompts=settings.target_images,\n",
        "        noise_prompt_seeds=[],\n",
        "        noise_prompt_weights=[],\n",
        "        size=[settings.width, settings.height],\n",
        "        init_image=settings.init_images,\n",
        "        init_weight=0.,\n",
        "        clip_model='ViT-B/32',\n",
        "        vqgan_config=settings.model[0],\n",
        "        vqgan_checkpoint=settings.model[1],\n",
        "        step_size=settings.step_size,\n",
        "        cutn=32,\n",
        "        cut_pow=1.,\n",
        "        display_freq=settings.images_interval,\n",
        "        seed=settings.seed,\n",
        "    )\n",
        "\n",
        "    return args"
      ],
      "id": "89ea225d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "531c03f3"
      },
      "source": [
        "#### Function generating images"
      ],
      "id": "531c03f3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "c6090826",
        "tags": []
      },
      "source": [
        "def generate_image(settings, args):\n",
        "    from urllib.request import urlopen\n",
        "    import re\n",
        "\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Using device:', device)\n",
        "    if settings.texts:\n",
        "        print('Using texts:', settings.texts)\n",
        "    if settings.target_images:\n",
        "        print('Using image prompts:', settings.target_images)\n",
        "    if args.seed is None:\n",
        "        seed = torch.seed()\n",
        "    else:\n",
        "        seed = args.seed\n",
        "    torch.manual_seed(seed)\n",
        "    print('Using seed:', seed)\n",
        "\n",
        "    model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "    perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "    # clock=deepcopy(perceptor.visual.positional_embedding.data)\n",
        "    # perceptor.visual.positional_embedding.data = clock/clock.max()\n",
        "    # perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n",
        "\n",
        "    cut_size = perceptor.visual.input_resolution\n",
        "\n",
        "    f = 2**(model.decoder.num_resolutions - 1)\n",
        "    make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "    toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "    sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        e_dim = 256\n",
        "        n_toks = model.quantize.n_embed\n",
        "        z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "        z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "    else:\n",
        "        e_dim = model.quantize.e_dim\n",
        "        n_toks = model.quantize.n_e\n",
        "        z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "        z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "    # z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    # z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "    # normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "    #                                            std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    if args.init_image:\n",
        "        if 'http' in args.init_image:\n",
        "            img = Image.open(urlopen(args.init_image))\n",
        "        else:\n",
        "            img = Image.open(args.init_image)\n",
        "        pil_image = img.convert('RGB')\n",
        "        pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "        pil_tensor = TF.to_tensor(pil_image)\n",
        "        z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n",
        "    else:\n",
        "        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "        # z = one_hot @ model.quantize.embedding.weight\n",
        "        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "            z = one_hot @ model.quantize.embed.weight\n",
        "        else:\n",
        "            z = one_hot @ model.quantize.embedding.weight\n",
        "        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
        "        z = torch.rand_like(z)*2\n",
        "    z_orig = z.clone()\n",
        "    z.requires_grad_(True)\n",
        "    opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                    std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "\n",
        "\n",
        "    pMs = []\n",
        "\n",
        "    for prompt in args.prompts:\n",
        "        txt, weight, stop = parse_prompt(prompt)\n",
        "        embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "        pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    for prompt in args.image_prompts:\n",
        "        path, weight, stop = parse_prompt(prompt)\n",
        "        img = Image.open(path)\n",
        "        pil_image = img.convert('RGB')\n",
        "        img = resize_image(pil_image, (sideX, sideY))\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = perceptor.encode_image(normalize(batch)).float()\n",
        "        pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "        pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "    def synth(z):\n",
        "        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "        else:\n",
        "            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "        return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def checkin(i, losses):\n",
        "        losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "        #tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "        out = synth(z)\n",
        "        #TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "        #display.display(display.Image('progress.png'))\n",
        "\n",
        "    def ascend_txt(n):\n",
        "        global i\n",
        "        out = synth(z)\n",
        "        iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "        \n",
        "        result = []\n",
        "\n",
        "        if args.init_weight:\n",
        "            # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "            result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*args.init_weight) / 2)\n",
        "        for prompt in pMs:\n",
        "            result.append(prompt(iii))\n",
        "        img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "        m = re.search(r'[\\S]*.png',settings.init_images)\n",
        "        short_init = m.group(0)\n",
        "\n",
        "        if (n in settings.iter_stops):\n",
        "            image_title = \"{text} on {init} (seed {seed}, step {step}) [iter {n}] [row {id}].png\".format(\n",
        "                text=settings.texts, init=short_init[:-4], n=n, seed=settings.seed, step=settings.step_size, id=settings.id)\n",
        "            imageio.imwrite('steps/full/' + image_title, np.array(img)) \n",
        "\n",
        "        if settings.gdrive_save and (n in settings.iter_stops):\n",
        "            image_title = \"{text} on {init} (seed {seed}, step {step}) [iter {n}] [row {id}].png\".format(\n",
        "                text=settings.texts, init=short_init[:-4], n=n, seed=settings.seed, step=settings.step_size, id=settings.id)\n",
        "            imageio.imwrite(settings.gdrive_path + image_title, np.array(img))       \n",
        "\n",
        "        return result\n",
        "\n",
        "    def train(i):\n",
        "        opt.zero_grad()\n",
        "        lossAll = ascend_txt(i)\n",
        "        if i % args.display_freq == 0:\n",
        "            checkin(i, lossAll)\n",
        "        \n",
        "        loss = sum(lossAll)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        with torch.no_grad():\n",
        "            z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "    i = 0\n",
        "    try:\n",
        "        with tqdm() as pbar:\n",
        "            while True:\n",
        "                train(i)\n",
        "                if i == settings.max_iterations:\n",
        "                    break\n",
        "                i += 1\n",
        "                pbar.update() \n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ],
      "id": "c6090826",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "f371d5b2"
      },
      "source": [
        "## Generate images"
      ],
      "id": "f371d5b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "733a589a"
      },
      "source": [
        "The code below generates images using the prepared model settings. It allows for multiple runs on many prompts and input images.\n",
        "\n",
        "If you decide to save images to your Google Drive, please specify the path. If not, leave empty."
      ],
      "id": "733a589a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "b50a4bca",
        "tags": [],
        "cellView": "form"
      },
      "source": [
        "#@title Launch parameters\n",
        "gdrive_save = False #@param {type:\"boolean\"}\n",
        "GDRIVE_PATH = '/your/gdrive/path' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "try:\n",
        "    runs_count = 0\n",
        "    \n",
        "    for run in multiple_runs[]:\n",
        "        for i in range(len(run.texts)):\n",
        "            temp_settings = AttrDict({'texts':run.texts[i],\n",
        "                                    'width':run.width,\n",
        "                                    'height':run.height,\n",
        "                                    'model':run.model,\n",
        "                                    'images_interval':run.images_interval,\n",
        "                                    'init_images':run.init_images[i],\n",
        "                                    'target_images':run.target_images,\n",
        "                                    'seed':run.seed,\n",
        "                                    'max_iterations':run.max_iterations,\n",
        "                                    'iter_stops':run.iter_stops,\n",
        "                                    'step_size':run.step_size,\n",
        "                                    'id':run.id[i],\n",
        "                                    'gdrive_save':gdrive_save,\n",
        "                                    'gdrive_path':GDRIVE_PATH,\n",
        "                                    })\n",
        "\n",
        "            arguments = prepare_model(temp_settings)\n",
        "            generate_image(temp_settings, arguments)\n",
        "\n",
        "        with open(\"batch_log.txt\",\"w+\") as f:\n",
        "            f.write(\"Finished run: {r}\".format(r=runs_count))\n",
        "        runs_count += 1\n",
        "\n",
        "except Exception as e:\n",
        "    with open(\"batch_error_log.txt\",\"w+\") as f:\n",
        "        f.write(\"{type}: {err} - {arg}\".format(type=str(type(e)), err=str(e), arg=str(e.args)))"
      ],
      "id": "b50a4bca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "8533560d"
      },
      "source": [
        "Code to .tar.gz the generated files' folder to easily download it to your computer."
      ],
      "id": "8533560d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "gradient": {
          "editing": false
        },
        "id": "042e0f99",
        "tags": []
      },
      "source": [
        "!GZIP=-9 tar chvfz name.tar.gz steps/full"
      ],
      "id": "042e0f99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2fd639a"
      },
      "source": [
        "# Creating canvas with generated images\n",
        "\n",
        "Once you have generated images you may want to present them in a more concise way. In order to do that you can use the code below - it will iterate the setps/full folder (or any folder you point towards) and create collages with images.\n",
        "\n",
        "First of all: point the folders where the images are and where you would like to save them (make sure the paths exist)"
      ],
      "id": "c2fd639a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b94b1ad"
      },
      "source": [
        "PATH = \"steps/full\"\n",
        "SAVE_PATH = \"canvas_folder\"\n",
        "\n",
        "# pandas limited the length of strings in cells, so we need to do that to get the whole filename into the dataframe\n",
        "pd.options.display.max_colwidth = 200"
      ],
      "id": "4b94b1ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ea0ddf"
      },
      "source": [
        "Now let's list the images we have. Sometimes a `.ipynb_checkopints` folder will be located, that's when we're remocing it. If you're using macOS sometimes you'll have `.DS_Store` file there as well."
      ],
      "id": "c1ea0ddf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d77ef8c1"
      },
      "source": [
        "file_list = os.listdir(PATH)\n",
        "len(file_list)\n",
        "file_list.sort(reverse=True)\n",
        "file_list.pop(0) # remove .ipynb_checkpoints\n",
        "file_list.pop(0) # remove .DS_Store"
      ],
      "id": "d77ef8c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3392d1de"
      },
      "source": [
        "Then we need our dataframe with all the files for generating canvas. Will be saving the filename in the last column for loading it to the canvas later."
      ],
      "id": "3392d1de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7643caa2"
      },
      "source": [
        "word_list = []\n",
        "\n",
        "for el in file_list:\n",
        "    if '.png' in el:\n",
        "        med_oth = ' '.join(el.split('by')[:1])\n",
        "        art = ' '.join(el.split(' by ')[1:])\n",
        "        try:\n",
        "            it = int(' '.join(el.split('iter ')[1:]).split(']')[0])\n",
        "        except TypeError as e:\n",
        "            print(\"TypeError: {arg}\".format(arg=e.args))\n",
        "        art = ' '.join(art.split(' on ')[:1])\n",
        "        name = (' '.join(el.split('on ')[1:])).split(' ')[0]\n",
        "\n",
        "\n",
        "        word_list.append([med_oth, art, it, name])\n",
        "\n",
        "df = pd.DataFrame(word_list, columns=['medium', 'artist', 'iterations', 'filename'])\n",
        "df['path'] = file_list\n",
        "\n",
        "df_c = df[df['artist'] != '']\n",
        "df_c.head()"
      ],
      "id": "7643caa2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a494c5ef"
      },
      "source": [
        "The drawing function for now is creating canvas, where the **rows** are artists, and **columns** are input files. It creates a canvas for each prompt separately, and divides artists into groups of size equal to the number of input files."
      ],
      "id": "a494c5ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9036dfa8"
      },
      "source": [
        "def canva_artist_filename(df):\n",
        "    ART = pd.unique(df[\"artist\"])\n",
        "    M_O = pd.unique(df[\"medium\"])\n",
        "    NAM = pd.unique(df[\"filename\"])\n",
        "    ITERS = pd.unique(df[\"iterations\"])\n",
        "    \n",
        "    A_L = len(ART)\n",
        "    M_L = len(M_O)\n",
        "    F_L = len(NAM) # this defines the size of canvas\n",
        "    I_L = len(ITERS)\n",
        "    n = 0\n",
        "    ART_GR = int(np.ceil(A_L/F_L))\n",
        "    MAX_IT = max(ITERS)\n",
        "    \n",
        "    hfont = {\"fontname\":\"Helvetica\"}\n",
        "    \n",
        "    for med in range(M_L):\n",
        "        plt.figure(figsize=(80,80))\n",
        "        for a_gr in range(ART_GR):\n",
        "            for fl in range(F_L):\n",
        "                for art in range(min(F_L,A_L-a_gr*F_L)):\n",
        "                    img_path = df[((df['filename'] == NAM[fl]) & (df['iterations'] == MAX_IT) & (df['medium'] == M_O[med]) & (df['artist'] == ART[art+(a_gr*F_L)]))]['path'].to_string(index=False)\n",
        "                    img = cv2.imread(os.path.join(PATH,img_path))\n",
        "                    try:\n",
        "                        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    except Exception as e:\n",
        "                        print(img_path)\n",
        "                        print(e)\n",
        "                        img_rgb = None\n",
        "                    print(n)\n",
        "                    n += 1\n",
        "\n",
        "                    ax = plt.subplot(F_L,F_L,art*F_L+fl+1)\n",
        "                    try:\n",
        "                        plt.imshow(img_rgb)\n",
        "                    except TypeError:\n",
        "                        pass\n",
        "\n",
        "                    if art == 0:\n",
        "                        ax.set_xlabel('\\n'+NAM[fl], va='top', fontsize=60, labelpad=100, **hfont)\n",
        "                        ax.xaxis.set_label_position('top')\n",
        "                    if fl == 0:\n",
        "                        plt.ylabel(str(ART[art+(a_gr*F_L)]), fontsize=60, rotation='horizontal', labelpad=200, **hfont)\n",
        "\n",
        "                    plt.rc('xtick', labelsize=0)\n",
        "                    plt.rc('ytick', labelsize=0)\n",
        "\n",
        "            plt.savefig(SAVE_PATH+'/'+'art_gr'+str(a_gr)+\"_\"+str(M_O[med])+\".png\", dpi=150, format='png')\n",
        "            print('Group {} finished!'.format(a_gr))"
      ],
      "id": "9036dfa8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa6b98f1"
      },
      "source": [
        "Finally, draw the canvas."
      ],
      "id": "fa6b98f1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c29d60e"
      },
      "source": [
        "canva_artist_filename(df_c)"
      ],
      "id": "3c29d60e",
      "execution_count": null,
      "outputs": []
    }
  ]
}