{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep-daze Fourier Feature Map",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keirwilliamsxyz/keirxyz/blob/main/deep_daze_Fourier_Feature_Map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Text to Image\n",
        "\n",
        "Based on: \n",
        "[CLIP](https://github.com/openai/CLIP) + [SIREN](https://github.com/vsitzmann/siren), colabs by [Ryan Murdock](https://rynmurdock.github.io/) and [@tg-bomze](https://github.com/tg-bomze)  \n",
        "by [eps696](https://github.com/eps696)\n",
        "\n",
        "## Features (optional)\n",
        "* using image and/or text as prompts\n",
        "* processing input coords with [Fourier feature mapping](https://github.com/tancik/fourier-feature-networks), making elements finer\n",
        "* few sampling modes, to play with composition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run this cell after each session restart**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\n",
        "\n",
        "import subprocess\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "try: \n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  from googletrans import Translator, constants\n",
        "  # from pprint import pprint\n",
        "  translator = Translator()\n",
        "except: pass\n",
        "!pip install ftfy\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from skimage import exposure\n",
        "from base64 import b64encode\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "# import glob\n",
        "from google.colab import output, files\n",
        "\n",
        "# from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "%cd /content/CLIP/\n",
        "import clip\n",
        "perceptor, preprocess = clip.load('ViT-B/32')\n",
        "\n",
        "workdir = '_out'\n",
        "tempdir = os.path.join(workdir, 'ttt')\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# # Libs\n",
        "\n",
        "class SineLayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
        "    super().__init__()\n",
        "    self.omega_0 = omega_0\n",
        "    self.is_first = is_first\n",
        "    self.in_features = in_features\n",
        "    self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "    self.init_weights()\n",
        "  \n",
        "  def init_weights(self):\n",
        "    with torch.no_grad():\n",
        "      if self.is_first:\n",
        "        lim = 1 / self.in_features\n",
        "      else:\n",
        "        lim = np.sqrt(6 / self.in_features) / self.omega_0\n",
        "      self.linear.weight.uniform_(-lim, lim)\n",
        "      \n",
        "  def forward(self, input):\n",
        "    return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "class Siren(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=True, \n",
        "                first_omega_0=30, hidden_omega_0=30.):\n",
        "    super().__init__()\n",
        "      \n",
        "    self.net = []\n",
        "    self.net.append(SineLayer(in_features, hidden_features, is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "    for i in range(hidden_layers):\n",
        "      self.net.append(SineLayer(hidden_features, hidden_features, is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "    if outermost_linear:\n",
        "      final_linear = nn.Linear(hidden_features, out_features)\n",
        "      with torch.no_grad():\n",
        "        lim = np.sqrt(6 / hidden_features) / hidden_omega_0\n",
        "        final_linear.weight.uniform_(-lim, lim)\n",
        "      self.net.append(final_linear)\n",
        "    else:\n",
        "      self.net.append(SineLayer(hidden_features, out_features, is_first=False, omega_0=hidden_omega_0))\n",
        "    \n",
        "    self.net = nn.Sequential(*self.net)\n",
        "  \n",
        "  def forward(self, coords):\n",
        "    coords = coords.clone().detach().requires_grad_(True)\n",
        "    output = self.net(coords.cuda())\n",
        "    return output.view(1, sideY, sideX, 3).permute(0, 3, 1, 2)#.sigmoid_()\n",
        "\n",
        "def get_mgrid(sideX, sideY):\n",
        "  tensors = [np.linspace(-1, 1, num=sideY), np.linspace(-1, 1, num=sideX)]\n",
        "  mgrid = np.stack(np.meshgrid(*tensors), axis=-1)\n",
        "  mgrid = mgrid.reshape(-1, 2) # dim 2\n",
        "  return mgrid\n",
        "\n",
        "# Preprocessing coords with Fourier feature mapping\n",
        "def fourierfm(xy, map=256, fourier_scale=4, mapping_type='gauss'):\n",
        "\n",
        "  def input_mapping(x, B): # feature mappings\n",
        "    x_proj = (2.*np.pi*x) @ B\n",
        "    y = np.concatenate([np.sin(x_proj), np.cos(x_proj)], axis=-1)\n",
        "    print(' mapping input:', x.shape, 'output', y.shape)\n",
        "    return y\n",
        "\n",
        "  if mapping_type == 'gauss': # Gaussian Fourier feature mappings\n",
        "    B = np.random.randn(2, map) \n",
        "    B *= fourier_scale # scale Gauss\n",
        "  else: # basic\n",
        "    B = np.eye(2).T\n",
        "\n",
        "  xy = input_mapping(xy, B)\n",
        "  return xy\n",
        "\n",
        "def slice_imgs(imgs, count, transform=None, uniform=False, micro=None):\n",
        "  def map(x, a, b):\n",
        "    return x * (b-a) + a\n",
        "\n",
        "  rnd_size = torch.rand(count)\n",
        "  if uniform is True:\n",
        "    rnd_offx = torch.rand(count)\n",
        "    rnd_offy = torch.rand(count)\n",
        "  else: # normal around center\n",
        "    rnd_offx = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1) \n",
        "    rnd_offy = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1)\n",
        "  \n",
        "  sz = [img.shape[2:] for img in imgs]\n",
        "  sz_min = [np.min(s) for s in sz]\n",
        "  if uniform is True:\n",
        "    sz = [[2*s[0], 2*s[1]] for s in list(sz)]\n",
        "    imgs = [pad_up_to(imgs[i], sz[i], type='centr') for i in range(len(imgs))]\n",
        "\n",
        "  sliced = []\n",
        "  for i, img in enumerate(imgs):\n",
        "    cuts = []\n",
        "    for c in range(count):\n",
        "      if micro is True: # both scales, micro mode\n",
        "        csize = map(rnd_size[c], 64, max(224, 0.25*sz_min[i])).int()\n",
        "      elif micro is False: # both scales, macro mode\n",
        "        csize = map(rnd_size[c], 0.5*sz_min[i], 0.98*sz_min[i]).int()\n",
        "      else: # single scale\n",
        "        csize = map(rnd_size[c], 64, 0.98*sz_min[i]).int()\n",
        "      offsetx = map(rnd_offx[c], 0, sz[i][1] - csize).int()\n",
        "      offsety = map(rnd_offy[c], 0, sz[i][0] - csize).int()\n",
        "      cut = img[:, :, offsety:offsety + csize, offsetx:offsetx + csize]\n",
        "      cut = torch.nn.functional.interpolate(cut, (224,224), mode='bicubic')\n",
        "      if transform is not None: \n",
        "        cut = transform(cut)\n",
        "      cuts.append(cut)\n",
        "    sliced.append(torch.cat(cuts, 0))\n",
        "  return sliced\n",
        "\n",
        "# def slice_imgs(imgs, count, transform=None, uniform=False):\n",
        "  # def map(x, a, b):\n",
        "  #   return x * (b-a) + a\n",
        "  # rnd_size = torch.rand(count)\n",
        "  # rnd_offx = torch.rand(count)\n",
        "  # rnd_offy = torch.rand(count)\n",
        "  \n",
        "  # sz = [img.shape[2:] for img in imgs]\n",
        "  # sz_min = [np.min(s) for s in sz]\n",
        "  # if uniform is True:\n",
        "  #   upsize = [[2*s[0], 2*s[1]] for s in list(sz)]\n",
        "  #   imgs = [pad_up_to(imgs[i], upsize[i], type='centr') for i in range(len(imgs))]\n",
        "\n",
        "  # sliced = []\n",
        "  # for i, img in enumerate(imgs):\n",
        "  #   cuts = []\n",
        "  #   for c in range(count):\n",
        "  #     csize = map(rnd_size[c], 0.5*sz_min[i], 0.98*sz_min[i]).int()\n",
        "  #     if uniform is True:\n",
        "  #       offsetx = map(rnd_offx[c], sz[i][1] - csize, 2* sz[i][1] - csize).int()\n",
        "  #       offsety = map(rnd_offy[c], sz[i][0] - csize, 2* sz[i][0] - csize).int()\n",
        "  #     else:\n",
        "  #       offsetx = map(rnd_offx[c], 0, sz[i][1] - csize).int()\n",
        "  #       offsety = map(rnd_offy[c], 0, sz[i][0] - csize).int()\n",
        "  #     cut = img[:, :, offsety:offsety + csize, offsetx:offsetx + csize]\n",
        "  #     cut = torch.nn.functional.interpolate(cut, (224,224), mode='bilinear')\n",
        "  #     if transform is not None: \n",
        "  #         cut = transform(cut)\n",
        "  #     cuts.append(cut)\n",
        "  #   sliced.append(torch.cat(cuts, 0))\n",
        "  # return sliced\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  out_sequence = seq_dir + '/%03d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
        "\n",
        "# Tiles an array around two points, allowing for pad lengths greater than the input length\n",
        "# adapted from https://discuss.pytorch.org/t/symmetric-padding/19866/3\n",
        "def tile_pad(xt, padding):\n",
        "  h, w = xt.shape[-2:]\n",
        "  left, right, top, bottom = padding\n",
        "\n",
        "  def tile(x, minx, maxx):\n",
        "    rng = maxx - minx\n",
        "    mod = np.remainder(x - minx, rng)\n",
        "    out = mod + minx\n",
        "    return np.array(out, dtype=x.dtype)\n",
        "\n",
        "  x_idx = np.arange(-left, w+right)\n",
        "  y_idx = np.arange(-top, h+bottom)\n",
        "  x_pad = tile(x_idx, -0.5, w-0.5)\n",
        "  y_pad = tile(y_idx, -0.5, h-0.5)\n",
        "  xx, yy = np.meshgrid(x_pad, y_pad)\n",
        "  return xt[..., yy, xx]\n",
        "\n",
        "def pad_up_to(x, size, type='centr'):\n",
        "  sh = x.shape[2:][::-1]\n",
        "  if list(x.shape[2:]) == list(size): return x\n",
        "  padding = []\n",
        "  for i, s in enumerate(size[::-1]):\n",
        "    if 'side' in type.lower():\n",
        "      padding = padding + [0, s-sh[i]]\n",
        "    else: # centr\n",
        "      p0 = (s-sh[i]) // 2\n",
        "      p1 = s-sh[i] - p0\n",
        "      padding = padding + [p0,p1]\n",
        "  y = tile_pad(x, padding)\n",
        "  return y\n",
        "\n",
        "class ProgressBar(object):\n",
        "  def __init__(self, task_num=10):\n",
        "    self.pbar = ipy.IntProgress(min=0, max=task_num, bar_style='') # (value=0, min=0, max=max, step=1, description=description, bar_style='')\n",
        "    self.labl = ipy.Label()\n",
        "    display(ipy.HBox([self.pbar, self.labl]))\n",
        "    self.task_num = task_num\n",
        "    self.completed = 0\n",
        "    self.start()\n",
        "\n",
        "  def start(self, task_num=None):\n",
        "    if task_num is not None:\n",
        "      self.task_num = task_num\n",
        "    if self.task_num > 0:\n",
        "      self.labl.value = '0/{}'.format(self.task_num)\n",
        "    else:\n",
        "      self.labl.value = 'completed: 0, elapsed: 0s'\n",
        "    self.start_time = time.time()\n",
        "\n",
        "  def upd(self, *p, **kw):\n",
        "    self.completed += 1\n",
        "    elapsed = time.time() - self.start_time + 0.0000000000001\n",
        "    fps = self.completed / elapsed if elapsed>0 else 0\n",
        "    if self.task_num > 0:\n",
        "      finaltime = time.asctime(time.localtime(self.start_time + self.task_num * elapsed / float(self.completed)))\n",
        "      fin = ' end %s' % finaltime[11:16]\n",
        "      percentage = self.completed / float(self.task_num)\n",
        "      eta = int(elapsed * (1 - percentage) / percentage + 0.5)\n",
        "      self.labl.value = '{}/{}, rate {:.3g}s, time {}s, left {}s, {}'.format(self.completed, self.task_num, 1./fps, shortime(elapsed), shortime(eta), fin)\n",
        "    else:\n",
        "      self.labl.value = 'completed {}, time {}s, {:.1f} steps/s'.format(self.completed, int(elapsed + 0.5), fps)\n",
        "    self.pbar.value += 1\n",
        "    if self.completed == self.task_num: self.pbar.bar_style = 'success'\n",
        "    return \n",
        "    # return self.completed\n",
        "\n",
        "def time_days(sec):\n",
        "  return '%dd %d:%02d:%02d' % (sec/86400, (sec/3600)%24, (sec/60)%60, sec%60)\n",
        "def time_hrs(sec):\n",
        "  return '%d:%02d:%02d' % (sec/3600, (sec/60)%60, sec%60)\n",
        "def shortime(sec):\n",
        "  if sec < 60:\n",
        "    time_short = '%d' % (sec)\n",
        "  elif sec < 3600:\n",
        "    time_short  = '%d:%02d' % ((sec/60)%60, sec%60)\n",
        "  elif sec < 86400:\n",
        "    time_short  = time_hrs(sec)\n",
        "  else:\n",
        "    time_short = time_days(sec)\n",
        "  return time_short\n",
        "\n",
        "!nvidia-smi -L\n",
        "print('\\nDone!')\n",
        "#@title Reinstall Custom Big-Sleep branch\n",
        "!yes | pip uninstall big-sleep\n",
        "# !pip install big-sleep --upgrade\n",
        "!pip install \"git+https://github.com/afiaka87/big-sleep.git@add_ema\"\n",
        "!nvidia-smi -L\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbJ9K4Cq8MtB"
      },
      "source": [
        "Type some text to hallucinate it, or upload some image to neuremix it.  \n",
        "Or use both, why not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Deep-Daze (SIREN + CLIP) w/ Fourier Feature Mapping\n",
        "#@markdown - Decrease `samples` (amount of random image cuts, trained per step) and/or `siren_layers` (depth/quality of the generator network), if facing OOM for higher resolutions.\n",
        "\n",
        "#@markdown - Try different `fourier_scale` values (shift to details), if using Fourier mapping option.  \n",
        "\n",
        "#@markdown - `sync_cut` option kinda makes it follow uploaded image composition (if there's any). `uniform` results in a more randomly tiled texture.\n",
        "\n",
        "\n",
        "#@markdown > CLIP Prompts\n",
        "text = \"shattered plates on the grass\" #@param {type:\"string\"}\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "#@markdown or \n",
        "upload_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "if translate:\n",
        "  text = translator.translate(text, dest='en').text\n",
        "if upload_image:\n",
        "  uploaded = files.upload()\n",
        "\n",
        "from tqdm import trange, tqdm\n",
        "from IPython.display import Image, display\n",
        "import random\n",
        "import torch\n",
        "\n",
        "#@markdown > Optional  Prompts\n",
        "fine_details = \"\" #@param {type:\"string\"}\n",
        "subtract = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Other CLIP Options \n",
        "translate = False #@param {type:\"boolean\"}\n",
        "invert = False #@param {type:\"boolean\"}\n",
        "sign = 1. if invert is True else -1\n",
        "\n",
        "if translate:\n",
        "  text = translator.translate(text, dest='en').text\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf tempdir\n",
        "if upload_image: # should at least have a 3 letter format, a dot, and one letter name\n",
        "  uploaded = image_path\n",
        "\n",
        "!mkdir -p $tempdir\n",
        "#@markdown > Dimensions\n",
        "sideX = 512 #@param {type:\"integer\"}\n",
        "sideY =   512#@param {type:\"integer\"}\n",
        "#@markdown > Symmetries\n",
        "uniform = True #@param {type:\"boolean\"}\n",
        "sync_cut = True #@param {type:\"boolean\"}\n",
        "#@markdown > Training\n",
        "steps = 700 #@param {type:\"integer\"}\n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "learning_rate = .00008 #@param {type:\"number\"}\n",
        "samples = 64 #@param {type:\"integer\"}\n",
        "#@markdown > Network\n",
        "siren_layers = 16 #@param {type:\"integer\"}\n",
        "use_fourier_feat_map = True #@param {type:\"boolean\"}\n",
        "fourier_maps = 128 #@param {type:\"integer\"}\n",
        "fourier_scale =  2 #@param {type:\"number\"}\n",
        "#@markdown > Misc\n",
        "audio_notification = True #@param {type:\"boolean\"}\n",
        "\n",
        "out_name = text.replace(' ', '_')\n",
        "\n",
        "mgrid = get_mgrid(sideY, sideX) # [262144,2]\n",
        "if use_fourier_feat_map:\n",
        "  mgrid = fourierfm(mgrid, fourier_maps, fourier_scale)\n",
        "mgrid = torch.from_numpy(mgrid.astype(np.float32)).cuda()\n",
        "\n",
        "model = Siren(mgrid.shape[-1], 256, siren_layers, 3).cuda()\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "img_enc = None\n",
        "if upload_image:\n",
        "  print(list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(input).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "  if sync_cut is True:\n",
        "    samples = samples // 2\n",
        "  else:\n",
        "    in_sliced = slice_imgs([img_in], samples, transform=norm_in, uniform=uniform)[0]\n",
        "    img_enc = perceptor.encode_image(in_sliced).detach().clone()\n",
        "    del img_in, in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 2:\n",
        "  print(' macro:', text)\n",
        "  tx = clip.tokenize(text)\n",
        "  txt_enc = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "\n",
        "if len(fine_details) > 0:\n",
        "  print(' micro:', fine_details)\n",
        "  tx2 = clip.tokenize(fine_details)\n",
        "  txt_enc2 = perceptor.encode_text(tx2.cuda()).detach().clone()\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  print(' without:', subtract)\n",
        "  tx0 = clip.tokenize(subtract)\n",
        "  txt_enc0 = perceptor.encode_text(tx0.cuda()).detach().clone()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "def displ(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = exposure.equalize_adapthist(np.clip(img, -1., 1.))\n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkin(num):\n",
        "  with torch.no_grad():\n",
        "    img = model(mgrid).cpu().numpy()[0]\n",
        "  displ(img, os.path.join(tempdir, '%03d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "def train(i, img_enc):\n",
        "  img_out = model(mgrid)\n",
        "  if upload_image and sync_cut is True:\n",
        "    imgs_sliced = slice_imgs([img_in, img_out], samples, norm_in, uniform)\n",
        "    img_enc = perceptor.encode_image(imgs_sliced[0])\n",
        "  else:\n",
        "    imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform)\n",
        "  out_enc = perceptor.encode_image(imgs_sliced[-1])\n",
        "  loss = 0\n",
        "  if upload_image:\n",
        "    loss += -100*torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 0: # input text\n",
        "      loss += sign * 100*torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  if len(subtract) > 0: # subtract text\n",
        "      loss += -sign * 100*torch.cosine_similarity(txt_enc0, out_enc, dim=-1).mean()\n",
        "  if len(fine_details) > 0: # input text for micro details\n",
        "      imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform=uniform, micro=True)\n",
        "      out_enc2 = perceptor.encode_image(imgs_sliced[-1])\n",
        "      loss += sign * 100*torch.cosine_similarity(txt_enc2, out_enc2, dim=-1).mean()\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkin(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i, img_enc)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "if audio_notification == True: output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}